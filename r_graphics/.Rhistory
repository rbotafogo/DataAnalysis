# Read train data
seguro_train <- fread('train.csv')
# seguro_train <- fread('train.csv', na.strings = c("-1", "-1.0"))
# eliminate non usefull calc predictors
calc = grep("calc", names(seguro_train))
seguro_train = seguro_train[, -calc, with=FALSE]
seguro_train$id = NULL
# Create dummy vars for variable ps_ind_03
seguro_train$ps_ind_03 = as.factor(seguro_train$ps_ind_03)
dum = dummyVars(~ps_ind_03, data = seguro_train, levelsOnly = TRUE)
cols = predict(dum, seguro_train)
colnames(cols) = c("pi0","pi1","pi2","pi3","pi4","pi5","pi6","pi7","pi8","pi9","pi10","pi11","pi12")
cols
colnames(cols) = c("pi0","pi1","pi2","pi3","pi4","pi5","pi6","pi7","pi8","pi9","pi10","pi11")
cols
seguro_train$ps_ind_03 = NULL
seguro_train = cbind(seguro_train, cols)
# Convert ps_reg_03 to two integer variables
t = sapply(seguro_train$ps_reg_03, convert)
seguro_train$RegA = t[1,]
seguro_train$RegM = t[2,]
# remove the ps_reg_03 predictor that we have just converted
seguro_train$ps_reg_03 = NULL
# get the target
seguro_train$target = as.factor(seguro_train$target)
y_train = seguro_train$target
seguro_train$target = NULL
levels(y_train) <- c("No", "Yes")
# Seguro_train is ready to be worked on
str(seguro_train)
seguro_train_cs = preProcess(seguro_train, method = c("center", "scale"))
clusters = kmeans(seguro_train, 5, iter.max = 1000)
clusters
c1_train = subset(seguro_train[, clusters == 1])
c1_train = subset(seguro_train, clusters == 1)
clusters$cluster
c1_train = subset(seguro_train, clusters$cluster == 1)
str(c1_train)
c1_target = y_train[clusters$cluster == 1]
c1_target
length(c1_target)
c2_train = subset(seguro_train, clusters$cluster == 2)
c2_target = y_train[clusters$cluster == 2]
c3_train = subset(seguro_train, clusters$cluster == 3)
c3_target = y_train[clusters$cluster == 3]
c4_train = subset(seguro_train, clusters$cluster == 4)
c4_target = y_train[clusters$cluster == 4]
c5_train = subset(seguro_train, clusters$cluster == 5)
c5_target = y_train[clusters$cluster == 5]
# create the training control object. Two-fold CV to keep the execution time under the kaggle limit. You can up this as your compute resources allow.
trControl = trainControl(
method = 'repeatedcv',
number = 5,
summaryFunction = giniSummary,
sampling = "up",
# sampling = "rose",
# sampling = "down",
classProbs = TRUE,
verboseIter = TRUE,
allowParallel = TRUE)
# create the tuning grid. Again keeping this small to avoid exceeding kernel memory limits.
# You can expand as your compute resources allow.
tuneGridXGB <- expand.grid(
nrounds=c(350),
max_depth = c(4),
eta = c(0.05),
gamma = c(0.05),
colsample_bytree = c(0.75),
subsample = c(0.50),
min_child_weight = c(100))
start <- Sys.time()
# train the xgboost learner
c1_train_xbg_fit <- train(
x = c1_train,
y = c1_target,
method = 'xgbTree',
metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB)
print(Sys.time() - start)
# Diagnostics
print(c1_train_xbg_fit$results)
print(xgbmod$resample)
print(c1_train_xbg_fit$resample)
start <- Sys.time()
# train the xgboost learner
c1_train_xbg_fit <- train(
x = c2_train,
y = c2_target,
method = 'xgbTree',
metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB)
print(Sys.time() - start)
# Diagnostics
print(c2_train_xbg_fit$results)
# Diagnostics
print(c1_train_xbg_fit$results)
print(c1_train_xbg_fit$resample)
# This is a minimal framework for training xgboost in R using caret to do the cross-validation/grid tuning and using the normalized gini metric for scoring. The
# of CV folds and size of the tuning grid are limited to remain under kaggle kernel limits.
# To improve the score up the nrounds and expand the tuning grid.
library(data.table)
library(caret)
library(xgboost)
library(verification)
library(ROSE)
# normalized gini function taked from:
# https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703
normalizedGini <- function(aa, pp) {
Gini <- function(a, p) {
if (length(a) !=  length(p)) stop("Actual and Predicted need to be equal lengths!")
temp.df <- data.frame(actual = a, pred = p, range=c(1:length(a)))
temp.df <- temp.df[order(-temp.df$pred, temp.df$range),]
population.delta <- 1 / length(a)
total.losses <- sum(a)
null.losses <- rep(population.delta, length(a)) # Hopefully is similar to accumulatedPopulationPercentageSum
accum.losses <- temp.df$actual / total.losses # Hopefully is similar to accumulatedLossPercentageSum
gini.sum <- cumsum(accum.losses - null.losses) # Not sure if this is having the same effect or not
sum(gini.sum) / length(a)
}
Gini(aa,pp) / Gini(aa,aa)
}
# create the normalized gini summary function to pass into caret
giniSummary <- function (data, lev = "Yes", model = NULL) {
levels(data$obs) <- c('0', '1')
out <- normalizedGini(as.numeric(levels(data$obs))[data$obs], data[, lev[2]])
names(out) <- "NormalizedGini"
out
}
convert = function(n) {
i = ceiling((n*40)**2)
for (a in 1:32) {
if (((i - a) %% 31) == 0) {
A = a;
}
M = (i - a) %/% 31;
}
return(c(A, M));
}
# Read train data
seguro_train <- fread('train.csv')
# seguro_train <- fread('train.csv', na.strings = c("-1", "-1.0"))
# eliminate non usefull calc predictors
calc = grep("calc", names(seguro_train))
seguro_train = seguro_train[, -calc, with=FALSE]
seguro_train$id = NULL
# Create dummy vars for variable ps_ind_03
seguro_train$ps_ind_03 = as.factor(seguro_train$ps_ind_03)
dum = dummyVars(~ps_ind_03, data = seguro_train, levelsOnly = TRUE)
cols = predict(dum, seguro_train)
cols
colnames(cols) = c("pi0","pi1","pi2","pi3","pi4","pi5","pi6","pi7","pi8","pi9","pi10","pi11")
seguro_train$ps_ind_03 = NULL
seguro_train = cbind(seguro_train, cols)
# Convert ps_reg_03 to two integer variables
t = sapply(seguro_train$ps_reg_03, convert)
seguro_train$RegA = t[1,]
seguro_train$RegM = t[2,]
# remove the ps_reg_03 predictor that we have just converted
seguro_train$ps_reg_03 = NULL
# get the target
seguro_train$target = as.factor(seguro_train$target)
y_train = seguro_train$target
seguro_train$target = NULL
levels(y_train) <- c("No", "Yes")
# Seguro_train is ready to be worked on
str(seguro_train)
seguro_train_cs = preProcess(seguro_train, method = c("center", "scale"))
clusters = kmeans(seguro_train, 3, iter.max = 1000)
clusters
c1_train = subset(seguro_train, clusters$cluster == 1)
c1_target = y_train[clusters$cluster == 1]
c2_train = subset(seguro_train, clusters$cluster == 2)
c2_target = y_train[clusters$cluster == 2]
c3_train = subset(seguro_train, clusters$cluster == 3)
c3_target = y_train[clusters$cluster == 3]
# create the training control object. Two-fold CV to keep the execution time under the kaggle limit. You can up this as your compute resources allow.
trControl = trainControl(
method = 'repeatedcv',
number = 5,
summaryFunction = giniSummary,
sampling = "up",
# sampling = "rose",
# sampling = "down",
classProbs = TRUE,
verboseIter = TRUE,
allowParallel = TRUE)
# create the tuning grid. Again keeping this small to avoid exceeding kernel memory limits.
# You can expand as your compute resources allow.
tuneGridXGB <- expand.grid(
nrounds=c(350),
max_depth = c(4),
eta = c(0.05),
gamma = c(0.05),
colsample_bytree = c(0.75),
subsample = c(0.50),
min_child_weight = c(100))
start <- Sys.time()
# train the xgboost learner
c1_train_xbg_fit <- train(
x = c1_train,
y = c1_target,
method = 'xgbTree',
metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB)
print(Sys.time() - start)
# Diagnostics
print(c1_train_xbg_fit$results)
print(c1_train_xbg_fit$resample)
start <- Sys.time()
# train the xgboost learner
c2_train_xbg_fit <- train(
x = c2_train,
y = c2_target,
method = 'xgbTree',
metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB)
print(Sys.time() - start)
# Diagnostics
print(c2_train_xbg_fit$results)
print(c1_train_xbg_fit$resample)
# Read train data
seguro_train <- fread('train.csv')
# seguro_train <- fread('train.csv', na.strings = c("-1", "-1.0"))
# eliminate non usefull calc predictors
calc = grep("calc", names(seguro_train))
seguro_train = seguro_train[, -calc, with=FALSE]
seguro_train$id = NULL
# Create dummy vars for variable ps_ind_03
# seguro_train$ps_ind_03 = as.factor(seguro_train$ps_ind_03)
# dum = dummyVars(~ps_ind_03, data = seguro_train, levelsOnly = TRUE)
# cols = predict(dum, seguro_train)
# colnames(cols) = c("pi0","pi1","pi2","pi3","pi4","pi5","pi6","pi7","pi8","pi9","pi10","pi11")
# seguro_train$ps_ind_03 = NULL
# seguro_train = cbind(seguro_train, cols)
# Convert ps_reg_03 to two integer variables
t = sapply(seguro_train$ps_reg_03, convert)
seguro_train$RegA = t[1,]
seguro_train$RegM = t[2,]
# remove the ps_reg_03 predictor that we have just converted
seguro_train$ps_reg_03 = NULL
# get the target
seguro_train$target = as.factor(seguro_train$target)
y_train = seguro_train$target
seguro_train$target = NULL
levels(y_train) <- c("No", "Yes")
# Seguro_train is ready to be worked on
str(seguro_train)
# seguro_train_cs = preProcess(seguro_train, method = c("center", "scale"))
clusters = kmeans(seguro_train, 3, iter.max = 1000)
clusters
c1_train = subset(seguro_train, clusters$cluster == 1)
c1_target = y_train[clusters$cluster == 1]
c2_train = subset(seguro_train, clusters$cluster == 2)
c2_target = y_train[clusters$cluster == 2]
c3_train = subset(seguro_train, clusters$cluster == 3)
c3_target = y_train[clusters$cluster == 3]
# create the training control object. Two-fold CV to keep the execution time under the kaggle limit. You can up this as your compute resources allow.
trControl = trainControl(
method = 'repeatedcv',
number = 5,
summaryFunction = giniSummary,
sampling = "up",
# sampling = "rose",
# sampling = "down",
classProbs = TRUE,
verboseIter = TRUE,
allowParallel = TRUE)
# create the tuning grid. Again keeping this small to avoid exceeding kernel memory limits.
# You can expand as your compute resources allow.
tuneGridXGB <- expand.grid(
nrounds=c(350),
max_depth = c(4),
eta = c(0.05),
gamma = c(0.05),
colsample_bytree = c(0.75),
subsample = c(0.50),
min_child_weight = c(100))
start <- Sys.time()
# train the xgboost learner
c1_train_xbg_fit <- train(
x = c1_train,
y = c1_target,
method = 'xgbTree',
metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB)
print(Sys.time() - start)
# Diagnostics
print(c1_train_xbg_fit$results)
print(c1_train_xbg_fit$resample)
start <- Sys.time()
# train the xgboost learner
c2_train_xbg_fit <- train(
x = c2_train,
y = c2_target,
method = 'xgbTree',
metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB)
print(Sys.time() - start)
# Diagnostics
print(c2_train_xbg_fit$results)
print(c1_train_xbg_fit$resample)
# This is a minimal framework for training xgboost in R using caret to do the cross-validation/grid tuning and using the normalized gini metric for scoring. The
# of CV folds and size of the tuning grid are limited to remain under kaggle kernel limits.
# To improve the score up the nrounds and expand the tuning grid.
library(data.table)
library(caret)
library(xgboost)
library(verification)
library(ROSE)
# normalized gini function taked from:
# https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703
normalizedGini <- function(aa, pp) {
Gini <- function(a, p) {
if (length(a) !=  length(p)) stop("Actual and Predicted need to be equal lengths!")
temp.df <- data.frame(actual = a, pred = p, range=c(1:length(a)))
temp.df <- temp.df[order(-temp.df$pred, temp.df$range),]
population.delta <- 1 / length(a)
total.losses <- sum(a)
null.losses <- rep(population.delta, length(a)) # Hopefully is similar to accumulatedPopulationPercentageSum
accum.losses <- temp.df$actual / total.losses # Hopefully is similar to accumulatedLossPercentageSum
gini.sum <- cumsum(accum.losses - null.losses) # Not sure if this is having the same effect or not
sum(gini.sum) / length(a)
}
Gini(aa,pp) / Gini(aa,aa)
}
# create the normalized gini summary function to pass into caret
giniSummary <- function (data, lev = "Yes", model = NULL) {
levels(data$obs) <- c('0', '1')
out <- normalizedGini(as.numeric(levels(data$obs))[data$obs], data[, lev[2]])
names(out) <- "NormalizedGini"
out
}
convert = function(n) {
i = ceiling((n*40)**2)
for (a in 1:32) {
if (((i - a) %% 31) == 0) {
A = a;
}
M = (i - a) %/% 31;
}
return(c(A, M));
}
# Read train data
seguro_train <- fread('train.csv')
# seguro_train <- fread('train.csv', na.strings = c("-1", "-1.0"))
# eliminate non usefull calc predictors
calc = grep("calc", names(seguro_train))
seguro_train = seguro_train[, -calc, with=FALSE]
seguro_train$id = NULL
str(seguro_train)
t = sapply(seguro_train$ps_reg_03, convert)
seguro_train$RegA = t[1,]
seguro_train$RegM = t[2,]
# remove the ps_reg_03 predictor that we have just converted
seguro_train$ps_reg_03 = NULL
seguro_train$target = as.factor(seguro_train$target)
str(seguro_train)
# create the training control object. Two-fold CV to keep the execution time under the kaggle limit. You can up this as your compute resources allow.
trControl = trainControl(
method = 'repeatedcv',
number = 5,
summaryFunction = giniSummary,
# sampling = "up",
# sampling = "rose",
# sampling = "down",
classProbs = TRUE,
verboseIter = TRUE,
allowParallel = TRUE)
# create the tuning grid. Again keeping this small to avoid exceeding kernel memory limits.
# You can expand as your compute resources allow.
tuneGridXGB <- expand.grid(
nrounds=c(350),
max_depth = c(4),
eta = c(0.05),
gamma = c(0.05),
colsample_bytree = c(0.75),
subsample = c(0.50),
min_child_weight = c(100))
y_train = seguro_train$target
seguro_train$target = NULL
str(seguro_train)
str(y_train)
levels(y_train) <- c("No", "Yes")
start <- Sys.time()
# train the xgboost learner
xgbmod <- train(
x = seguro_train,
y = y_train,
method = 'xgbTree',
metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB)
print(Sys.time() - start)
# Diagnostics
print(xgbmod$results)
print(xgbmod$resample)
# plot results (useful for larger tuning grids)
plot(xgbmod)
xgbmod
seguro_test <- fread('test.csv')
test_id = seguro_test$id
seguro_test$id = NULL
# x_test <- seguro_train[!train_index, 3:ncol(seguro_train)]
t = sapply(seguro_test$ps_reg_03, convert)
seguro_test$RegA = t[1,]
seguro_test$RegM = t[2,]
calc = grep("calc", names(seguro_test))
seguro_test = seguro_test[, -calc, with=FALSE]
str(seguro_test)
seguro_test$ps_reg_03 = NULL
str(seguro_test)
preds_final <- predict(xgbmod, newdata = seguro_test, type = "prob")
# prep the predictions for submissions
sub <- data.frame(id = as.integer(test_id), target = preds_final$Yes)
# write to csv
write.csv(sub, '20171110xgb_v1.csv', row.names = FALSE)
# create the training control object. Two-fold CV to keep the execution time under the kaggle limit. You can up this as your compute resources allow.
trControl = trainControl(
method = 'repeatedcv',
number = 5,
summaryFunction = giniSummary,
# sampling = "up",
# sampling = "rose",
# sampling = "down",
classProbs = TRUE,
verboseIter = TRUE,
allowParallel = TRUE)
# create the tuning grid. Again keeping this small to avoid exceeding kernel memory limits.
# You can expand as your compute resources allow.
tuneGridXGB <- expand.grid(
nrounds=c(350),
max_depth = c(4),
eta = c(0.025, 0.05),
gamma = c(0.05),
colsample_bytree = c(0.75),
subsample = c(0.25, 0.50),
min_child_weight = c(50, 100))
start <- Sys.time()
# train the xgboost learner
xgbmod <- train(
x = seguro_train,
y = y_train,
method = 'xgbTree',
metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB)
print(Sys.time() - start)
# Diagnostics
print(xgbmod$results)
print(xgbmod$resample)
# plot results (useful for larger tuning grids)
plot(xgbmod)
xgbmod
# create the training control object. Two-fold CV to keep the execution time under the kaggle limit. You can up this as your compute resources allow.
trControl = trainControl(
method = 'repeatedcv',
number = 5,
summaryFunction = giniSummary,
# sampling = "up",
# sampling = "rose",
# sampling = "down",
classProbs = TRUE,
verboseIter = TRUE,
allowParallel = TRUE)
# create the tuning grid. Again keeping this small to avoid exceeding kernel memory limits.
# You can expand as your compute resources allow.
tuneGridXGB <- expand.grid(
nrounds=c(350),
max_depth = c(4),
eta = c(0.05, 0.1),
gamma = c(0.05),
colsample_bytree = c(0.75),
subsample = c(0.50, 0.75),
min_child_weight = c(100, 150))
start <- Sys.time()
# train the xgboost learner
xgbmod <- train(
x = seguro_train,
y = y_train,
method = 'xgbTree',
metric = 'NormalizedGini',
trControl = trControl,
tuneGrid = tuneGridXGB)
print(Sys.time() - start)
# Diagnostics
print(xgbmod$results)
print(xgbmod$resample)
# plot results (useful for larger tuning grids)
plot(xgbmod)
xgbmod
seguro_test <- fread('test.csv')
test_id = seguro_test$id
seguro_test$id = NULL
# x_test <- seguro_train[!train_index, 3:ncol(seguro_train)]
t = sapply(seguro_test$ps_reg_03, convert)
seguro_test$RegA = t[1,]
seguro_test$RegM = t[2,]
calc = grep("calc", names(seguro_test))
seguro_test = seguro_test[, -calc, with=FALSE]
seguro_test$ps_reg_03 = NULL
str(seguro_test)
preds_final <- predict(xgbmod, newdata = seguro_test, type = "prob")
# prep the predictions for submissions
sub <- data.frame(id = as.integer(test_id), target = preds_final$Yes)
# write to csv
write.csv(sub, '20171110xgb_v2.csv', row.names = FALSE)
